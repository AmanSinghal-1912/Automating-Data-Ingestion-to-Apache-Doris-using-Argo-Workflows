# CSV to Doris Pipeline - Complete Flow Documentation

## ğŸ“‹ Table of Contents
1. [System Architecture Overview](#system-architecture-overview)
2. [Detailed Flow Diagram](#detailed-flow-diagram)
3. [Component Breakdown](#component-breakdown)
4. [Step-by-Step Execution](#step-by-step-execution)
5. [Data Flow & Transformations](#data-flow--transformations)
6. [Error Handling Flow](#error-handling-flow)
7. [Kubernetes & Argo Integration](#kubernetes--argo-integration)

---

## ğŸ—ï¸ System Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     WINDOWS HOST MACHINE                             â”‚
â”‚  C:\Users\singh\Desktop\Minikube-Doris\                             â”‚
â”‚                                                                       â”‚
â”‚  â”œâ”€â”€ data/               (Source CSV files)                          â”‚
â”‚  â”œâ”€â”€ scripts/            (Python pipeline scripts)                   â”‚
â”‚  â”œâ”€â”€ stage_test/         (Staged/transformed CSV files)              â”‚
â”‚  â”œâ”€â”€ error_files/        (Failed rows & schema mismatches)           â”‚
â”‚  â”œâ”€â”€ checkpoint.txt      (Tracks processed files)                    â”‚
â”‚  â””â”€â”€ table_map.json      (Maps schemas to table names)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ Volume Mount (hostPath)
                    â”‚ /Minikube-Doris â†” C:\Users\singh\Desktop\Minikube-Doris
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MINIKUBE KUBERNETES                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         ARGO WORKFLOWS NAMESPACE (argo)                      â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚
â”‚  â”‚  â”‚  CronWorkflow: csv-doris-cron                   â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  Schedule: */5 * * * * (Every 5 minutes)        â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  Timezone: America/New_York                     â”‚         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚  â”‚                     â”‚ Creates workflow every 5 min            â”‚   â”‚
â”‚  â”‚                     â†“                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚
â”‚  â”‚  â”‚  Workflow Instance: csv-doris-cron-XXXXXXXXXX   â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  (Ephemeral - deleted after success/failure)     â”‚         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚  â”‚                     â”‚ Spawns Pod                              â”‚   â”‚
â”‚  â”‚                     â†“                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚
â”‚  â”‚  â”‚  Pod: csv-doris-cron-XXXXXXXXXX                 â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  Image: python:3.11-slim                        â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  Init: Install dependencies               â”‚  â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  - pip install pandas pymysql requests    â”‚  â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  Main: Run pipeline_local.py              â”‚  â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  - Orchestrates all pipeline steps        â”‚  â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  Volumes:                                        â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  - /app â†’ mounted from host                     â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  Environment:                                    â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  - DORIS_HOST=host.docker.internal              â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  - DORIS_PORT=9030                               â”‚         â”‚   â”‚
â”‚  â”‚  â”‚  - DORIS_DB=updated_test2                        â”‚         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ MySQL Protocol (Port 9030)
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     APACHE DORIS DATABASE                            â”‚
â”‚  IP: 192.168.29.181                                                  â”‚
â”‚  Ports: 9030 (MySQL), 8030 (HTTP/FE)                                â”‚
â”‚  Database: updated_test2                                             â”‚
â”‚  Table: main_data_table                                              â”‚
â”‚  Model: DUPLICATE KEY                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Detailed Flow Diagram

```
START (Every 5 minutes - Cron: */5 * * * *)
  â”‚
  â”‚ Argo CronWorkflow Controller triggers
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: POD INITIALIZATION                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â†’ Create Pod: csv-doris-cron-XXXXXXXXXX
  â”‚   â””â”€â†’ Pull Image: python:3.11-slim
  â”‚
  â”œâ”€â†’ Mount Volume: /app â† C:\Users\singh\Desktop\Minikube-Doris
  â”‚
  â”œâ”€â†’ Set Environment Variables:
  â”‚   â”œâ”€ DORIS_HOST=host.docker.internal
  â”‚   â”œâ”€ DORIS_PORT=9030
  â”‚   â”œâ”€ DORIS_USER=root
  â”‚   â”œâ”€ DORIS_PASS=(empty)
  â”‚   â”œâ”€ DORIS_DB=updated_test2
  â”‚   â””â”€ DORIS_FE_HTTP_PORT=8030
  â”‚
  â””â”€â†’ Install Dependencies:
      â””â”€â†’ pip install pandas pymysql requests numpy
  â”‚
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: PIPELINE ORCHESTRATION (pipeline_local.py)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â†’ Print Banner:
  â”‚   ======================================================================
  â”‚     *** ARGO CRON WORKFLOW STARTED ***
  â”‚     Timestamp: 2025-11-06 06:45:46
  â”‚   ======================================================================
  â”‚
  â”œâ”€â†’ [STEP 1] DISCOVER ALL CSV FILES (0_ingest.py)
  â”‚   â”‚
  â”‚   â”œâ”€â†’ Scan directory: /app/data/*.csv
  â”‚   â”œâ”€â†’ Found: ['a.csv', 'b.csv', 'c.csv', 'd.csv', 'e.csv', 
  â”‚   â”‚            'emp.csv', 'meal_metadata.csv', 'people.csv']
  â”‚   â””â”€â†’ Return: 8 files discovered
  â”‚
  â”œâ”€â†’ [STEP 2] CHECK PROCESSED FILES
  â”‚   â”‚
  â”‚   â”œâ”€â†’ Read: /app/checkpoint.txt
  â”‚   â”œâ”€â†’ Already processed: 0 files
  â”‚   â””â”€â†’ Remaining to process: 8 files
  â”‚
  â”œâ”€â†’ [LOOP] FOR EACH UNPROCESSED FILE:
  â”‚   â”‚
  â”‚   â””â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚       â”‚  FILE PROCESSING WORKFLOW                             â”‚
  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚       â”‚
  â”‚       â”œâ”€â†’ [STEP 2.1] DISCOVER NEXT FILE (discover_next_1.py)
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€â†’ Read: /app/checkpoint.txt
  â”‚       â”‚   â”œâ”€â†’ Find first unprocessed file
  â”‚       â”‚   â””â”€â†’ Return: "a.csv" (for example)
  â”‚       â”‚
  â”‚       â”œâ”€â†’ [STEP 2.2] VALIDATE (2_validate.py)
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€â†’ Read: /app/data/a.csv
  â”‚       â”‚   â”œâ”€â†’ Check: File exists? âœ“
  â”‚       â”‚   â”œâ”€â†’ Check: Valid CSV format? âœ“
  â”‚       â”‚   â”œâ”€â†’ Check: Has data? âœ“
  â”‚       â”‚   â””â”€â†’ Return: VALID
  â”‚       â”‚
  â”‚       â”œâ”€â†’ [STEP 2.3] TRANSFORM & STAGE (3_transform.py)
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€â†’ Read: /app/data/a.csv
  â”‚       â”‚   â”‚   Input: 6 rows, 2 columns ['name', 'age']
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€â†’ Clean column names:
  â”‚       â”‚   â”‚   'name' â†’ 'name'
  â”‚       â”‚   â”‚   'age' â†’ 'age'
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€â†’ Remove duplicates: 0 duplicates found
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€â†’ Handle nulls: 0 nulls found
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€â†’ Save staged file:
  â”‚       â”‚   â”‚   Output: /app/stage_test/staged_a.csv
  â”‚       â”‚   â”‚   Final: 6 rows, 2 columns
  â”‚       â”‚   â”‚
  â”‚       â”‚   â””â”€â†’ Return: /app/stage_test/staged_a.csv
  â”‚       â”‚
  â”‚       â”œâ”€â†’ [STEP 2.4] LOAD TO DORIS (4_load_to_doris.py)
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€â†’ Read: /app/stage_test/staged_a.csv
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€â†’ CHECK: Is this the FIRST file?
  â”‚       â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”œâ”€â†’ YES (table_map.json doesn't exist)
  â”‚       â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”œâ”€â†’ [SCHEMA DETECTION]
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Analyze 'name' column:
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   Values: ['Kara', 'Ishuu', 'Sharma', ...]
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   Type: VARCHAR(100)
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Analyze 'age' column:
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   Values: [77, 91, 55, 22, 19, 66]
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   All numeric? YES (100%)
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   Range: 19-91
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   Type: TINYINT (-128 to 127)
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”‚   â””â”€â†’ Schema: age|name (sorted column keys)
  â”‚       â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”œâ”€â†’ [CREATE TABLE]
  â”‚       â”‚   â”‚   â”‚   â”‚   CREATE TABLE main_data_table (
  â”‚       â”‚   â”‚   â”‚   â”‚     `id` BIGINT NOT NULL,
  â”‚       â”‚   â”‚   â”‚   â”‚     `name` VARCHAR(100),
  â”‚       â”‚   â”‚   â”‚   â”‚     `age` TINYINT
  â”‚       â”‚   â”‚   â”‚   â”‚   )
  â”‚       â”‚   â”‚   â”‚   â”‚   DUPLICATE KEY(`id`)
  â”‚       â”‚   â”‚   â”‚   â”‚   DISTRIBUTED BY HASH(`id`) BUCKETS 3
  â”‚       â”‚   â”‚   â”‚   â”‚   PROPERTIES ("replication_num" = "1");
  â”‚       â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”œâ”€â†’ Save table_map.json:
  â”‚       â”‚   â”‚   â”‚   â”‚   {
  â”‚       â”‚   â”‚   â”‚   â”‚     "main_table": "main_data_table",
  â”‚       â”‚   â”‚   â”‚   â”‚     "main_schema": "age|name"
  â”‚       â”‚   â”‚   â”‚   â”‚   }
  â”‚       â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â””â”€â†’ Set last_id = 0
  â”‚       â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â””â”€â†’ NO (table_map.json exists)
  â”‚       â”‚   â”‚       â”‚
  â”‚       â”‚   â”‚       â”œâ”€â†’ Read table_map.json
  â”‚       â”‚   â”‚       â”œâ”€â†’ Expected schema: "age|name"
  â”‚       â”‚   â”‚       â”œâ”€â†’ Current schema: "age|name" (from staged file)
  â”‚       â”‚   â”‚       â”‚
  â”‚       â”‚   â”‚       â”œâ”€â†’ Schema match? 
  â”‚       â”‚   â”‚       â”‚   â”‚
  â”‚       â”‚   â”‚       â”‚   â”œâ”€â†’ YES: Continue
  â”‚       â”‚   â”‚       â”‚   â”‚   â””â”€â†’ Get max(id) from main_data_table
  â”‚       â”‚   â”‚       â”‚   â”‚       last_id = 6 (for example)
  â”‚       â”‚   â”‚       â”‚   â”‚
  â”‚       â”‚   â”‚       â”‚   â””â”€â†’ NO: SCHEMA MISMATCH!
  â”‚       â”‚   â”‚       â”‚       â”œâ”€â†’ Save to: /app/error_files/error_b.csv
  â”‚       â”‚   â”‚       â”‚       â”œâ”€â†’ Log: Schema mismatch detected
  â”‚       â”‚   â”‚       â”‚       â”œâ”€â†’ Checkpoint file anyway
  â”‚       â”‚   â”‚       â”‚       â””â”€â†’ Skip to next file
  â”‚       â”‚   â”‚       â”‚
  â”‚       â”‚   â”‚       â””â”€â†’ Continue loading...
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€â†’ [ADD AUTO-INCREMENT IDs]
  â”‚       â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”œâ”€â†’ Insert 'id' column at position 0
  â”‚       â”‚   â”‚   â””â”€â†’ IDs: range(last_id+1, last_id+1+len(df))
  â”‚       â”‚   â”‚       Example: [1, 2, 3, 4, 5, 6] for first file
  â”‚       â”‚   â”‚                [7, 8, 9, 10, 11] for second file
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€â†’ [ROW-LEVEL VALIDATION]
  â”‚       â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”œâ”€â†’ Connect to Doris
  â”‚       â”‚   â”‚   â”œâ”€â†’ Get table schema (DESC main_data_table):
  â”‚       â”‚   â”‚   â”‚   id: BIGINT
  â”‚       â”‚   â”‚   â”‚   name: VARCHAR(100)
  â”‚       â”‚   â”‚   â”‚   age: TINYINT
  â”‚       â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”œâ”€â†’ FOR EACH ROW in dataframe:
  â”‚       â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”œâ”€â†’ FOR EACH COLUMN:
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Is NULL/NaN? â†’ Store as NULL
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Expected type INT/BIGINT/TINYINT?
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Try: int(float(str(value)))
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Success? â†’ Store as integer
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â†’ Fail? â†’ Mark row as BAD
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚       Example: "twenty" cannot convert to int
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Expected type FLOAT/DOUBLE?
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Try: float(str(value))
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Success? â†’ Store as float
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â†’ Fail? â†’ Mark row as BAD
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Expected type DATE/DATETIME?
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Try: pd.to_datetime(value)
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Success? â†’ Store as date
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â†’ Fail? â†’ Mark row as BAD
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”‚   â””â”€â†’ Expected type VARCHAR?
  â”‚       â”‚   â”‚   â”‚   â”‚       â””â”€â†’ Accept anything
  â”‚       â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â”œâ”€â†’ Row valid? 
  â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ YES: Add to good_rows[]
  â”‚       â”‚   â”‚   â”‚   â”‚   â””â”€â†’ NO: Add to bad_rows[]
  â”‚       â”‚   â”‚   â”‚   â”‚       Log: [WARN] Row 3 invalid: Column 'age' expects INT, got 'twenty'
  â”‚       â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”œâ”€â†’ Any bad rows?
  â”‚       â”‚   â”‚   â”‚   â”œâ”€â†’ YES:
  â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Save to: /app/error_files/error_e.csv
  â”‚       â”‚   â”‚   â”‚   â”‚   â”‚   (Contains only the bad rows)
  â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â†’ Log: Found 2 bad rows - saved to error_e.csv
  â”‚       â”‚   â”‚   â”‚   â”‚   â””â”€â†’ Continue with good rows only
  â”‚       â”‚   â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”‚   â””â”€â†’ NO: All rows valid!
  â”‚       â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â””â”€â†’ Result:
  â”‚       â”‚   â”‚       good_rows: [(1, 'lara', 77), (2, 'Aara', 21)]
  â”‚       â”‚   â”‚       bad_rows: [(3, 'aysuh', 'twenty'), (4, 'rahul', 'fifty')]
  â”‚       â”‚   â”‚
  â”‚       â”‚   â”œâ”€â†’ [INSERT TO DORIS]
  â”‚       â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”œâ”€â†’ Build SQL:
  â”‚       â”‚   â”‚   â”‚   INSERT INTO `main_data_table` 
  â”‚       â”‚   â”‚   â”‚   (`id`, `name`, `age`) 
  â”‚       â”‚   â”‚   â”‚   VALUES (%s, %s, %s)
  â”‚       â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”œâ”€â†’ Execute batch insert (only good rows):
  â”‚       â”‚   â”‚   â”‚   executemany(sql, good_rows)
  â”‚       â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â”œâ”€â†’ Commit transaction
  â”‚       â”‚   â”‚   â”‚
  â”‚       â”‚   â”‚   â””â”€â†’ Log: Successfully loaded 2 rows
  â”‚       â”‚   â”‚       (Skipped 2 bad rows)
  â”‚       â”‚   â”‚
  â”‚       â”‚   â””â”€â†’ Return: SUCCESS
  â”‚       â”‚
  â”‚       â””â”€â†’ [STEP 2.5] CHECKPOINT (6_checkpoint.py)
  â”‚           â”‚
  â”‚           â”œâ”€â†’ Read: /app/checkpoint.txt
  â”‚           â”œâ”€â†’ Append: "a.csv\n"
  â”‚           â””â”€â†’ Save: /app/checkpoint.txt
  â”‚               Now contains: a.csv
  â”‚
  â”œâ”€â†’ LOOP continues for next unprocessed file...
  â”‚
  â””â”€â†’ [END LOOP] All files processed!
  â”‚
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: SUMMARY & CLEANUP                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â†’ Print Summary:
  â”‚   ======================================================================
  â”‚     *** PIPELINE COMPLETED SUCCESSFULLY ***
  â”‚   ======================================================================
  â”‚   Total runtime: 60.45 seconds
  â”‚   Files processed: 6
  â”‚   Schema mismatch errors: 2 files
  â”‚   Bad rows skipped: 2 rows
  â”‚   ======================================================================
  â”‚
  â”œâ”€â†’ Exit Code: 0 (Success)
  â”‚
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: ARGO WORKFLOW CLEANUP                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â†’ Workflow Status: Succeeded âœ“
  â”œâ”€â†’ Pod Status: Completed
  â”œâ”€â†’ Keep workflow history: 3 successful, 3 failed
  â””â”€â†’ Delete pod after completion
  â”‚
  â†“
WAIT FOR NEXT CRON TRIGGER (5 minutes)
  â”‚
  â””â”€â†’ Loop back to START...
```

---

## ğŸ§© Component Breakdown

### **1. Argo CronWorkflow (argo-cron-pipeline.yaml)**

**Location**: `C:\Users\singh\Desktop\Minikube-Doris\argo-cron-pipeline.yaml`

**Purpose**: Kubernetes CronWorkflow that triggers the pipeline every 5 minutes.

**Key Configuration**:
```yaml
apiVersion: argoproj.io/v1alpha1
kind: CronWorkflow
metadata:
  name: csv-doris-cron
  namespace: argo
spec:
  schedule: "*/5 * * * *"    # Every 5 minutes
  timezone: "America/New_York"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
```

**Cron Schedule Breakdown**:
```
*/5  *  *  *  *
 â”‚   â”‚  â”‚  â”‚  â”‚
 â”‚   â”‚  â”‚  â”‚  â””â”€â”€â”€ Day of week (0-6, Sunday=0)
 â”‚   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€ Month (1-12)
 â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ Day of month (1-31)
 â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hour (0-23)
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Minute (0-59, */5 = every 5 minutes)
```

**What happens**:
- Argo Workflows controller watches CronWorkflows
- Every 5 minutes (at :00, :05, :10, :15, etc.), it creates a new Workflow
- Workflow creates a Pod to run the pipeline
- Pod mounts volume, installs dependencies, runs pipeline
- After completion, pod is deleted (logs preserved in Argo)

---

### **2. Pipeline Scripts**

#### **pipeline_local.py** - Master Orchestrator
**Role**: Main entry point, coordinates all pipeline steps

**Key Functions**:
- `log_step(message, level)`: Timestamps all log messages
- `run(cmd, desc)`: Executes subprocess commands with error handling
- Main loop: Discovers files, validates, transforms, loads, checkpoints

**Flow**:
1. Print workflow start banner
2. Discover all CSV files
3. Check checkpoint for processed files
4. FOR EACH unprocessed file:
   - Discover next file
   - Validate
   - Transform
   - Load to Doris (with error handling)
   - Checkpoint
5. Print summary statistics

---

#### **0_ingest.py** - File Discovery
**Role**: Scans data directory for all CSV files

**Input**: None (reads from `/app/data/`)

**Output**: Prints list of CSV files found

**Logic**:
```python
csv_files = glob.glob(os.path.join(CSV_DIR, "*.csv"))
print(f"Found: {[os.path.basename(f) for f in csv_files]}")
```

---

#### **discover_next_1.py** - Next File Selector
**Role**: Finds the next unprocessed CSV file

**Input**: 
- Reads `/app/checkpoint.txt` (processed files)
- Reads `/app/data/*.csv` (all files)

**Output**: Filename of next unprocessed file (or empty if all done)

**Logic**:
```python
all_files = set(['a.csv', 'b.csv', 'c.csv', ...])
processed = set(['a.csv', 'c.csv'])  # from checkpoint.txt
remaining = all_files - processed  # {'b.csv', ...}
return sorted(remaining)[0] if remaining else None
```

---

#### **2_validate.py** - File Validation
**Role**: Ensures CSV file is valid before processing

**Input**: Filename (e.g., "a.csv")

**Checks**:
- âœ“ File exists
- âœ“ File is readable
- âœ“ Valid CSV format
- âœ“ Has at least 1 row of data
- âœ“ Has at least 1 column

**Output**: Prints "Validated: <filename>" or raises error

---

#### **3_transform.py** - Data Cleaning & Staging
**Role**: Clean and standardize data before loading

**Input**: Filename (e.g., "a.csv")

**Transformations**:
1. **Column name cleaning**:
   - `strip()` - remove whitespace
   - `lower()` - convert to lowercase
   - `replace(' ', '_')` - spaces to underscores
   - `replace('(', '').replace(')', '')` - remove parentheses

2. **Remove duplicate rows**:
   - `df.drop_duplicates()`

3. **Handle missing values**:
   - Fill NULL values with string "NULL"
   - Log which columns had nulls

**Output**: 
- Creates `/app/stage_test/staged_<filename>.csv`
- Prints transformation statistics
- Returns path to staged file

**Example**:
```
Input:  data/a.csv          â†’ 6 rows, ['name', 'age']
Output: stage_test/staged_a.csv â†’ 6 rows, ['name', 'age']
```

---

#### **4_load_to_doris.py** - Database Loading (MOST COMPLEX)
**Role**: Intelligent schema detection, row validation, data loading

**Input**: 
- Staged file path: `/app/stage_test/staged_a.csv`
- Original filename: `a.csv`

**Key Functions**:

##### **infer_doris_type(series)** - Intelligent Type Detection
**Purpose**: Determine the best Doris data type for a column

**Algorithm**:
```
1. Remove NULL/NaN values from series

2. If column is object/string type:
   - Count how many values are numeric (can convert to float)
   - If > 50% are numeric â†’ treat as NUMERIC
   - Else â†’ treat as VARCHAR
   
3. If column is numeric:
   - Check if all floats are actually integers (22.0 â†’ 22)
   - Determine range:
     â€¢ -128 to 127 â†’ TINYINT
     â€¢ -32768 to 32767 â†’ SMALLINT
     â€¢ -2147483648 to 2147483647 â†’ INT
     â€¢ Larger â†’ BIGINT
   - If has decimals â†’ DOUBLE
   
4. If column is boolean â†’ BOOLEAN

5. If column is datetime â†’ DATETIME

6. If column is string:
   - Calculate max length
   - Assign VARCHAR with appropriate size:
     â€¢ â‰¤50 chars â†’ VARCHAR(100)
     â€¢ â‰¤100 chars â†’ VARCHAR(200)
     â€¢ â‰¤255 chars â†’ VARCHAR(500)
     â€¢ â‰¤1000 chars â†’ VARCHAR(2000)
     â€¢ Larger â†’ VARCHAR(65533)
```

**Example**:
```python
# Column 'age' with values: [77, 91, 55, 22, 19, 66]
infer_doris_type(df['age'])
â†’ All numeric, range 19-91
â†’ Returns: "TINYINT"

# Column 'name' with values: ['Kara', 'Ishuu', 'Sharma']
infer_doris_type(df['name'])
â†’ Max length: 6 characters
â†’ Returns: "VARCHAR(100)"

# Column with mixed: [77, 91, "twenty", 55]
infer_doris_type(df['mixed'])
â†’ 75% numeric (3/4), but > 50%
â†’ Converts to numeric, coerces "twenty" to NaN
â†’ After dropping NaN: [77, 91, 55]
â†’ Returns: "TINYINT"
```

---

##### **First File Processing** - Table Creation
**When**: table_map.json doesn't exist (first file ever)

**Steps**:
1. Create table name: `main_data_table`
2. Detect schema using `infer_doris_type()` for each column
3. Create table in Doris:
   ```sql
   CREATE TABLE main_data_table (
     `id` BIGINT NOT NULL,
     `name` VARCHAR(100),
     `age` TINYINT
   )
   DUPLICATE KEY(`id`)
   DISTRIBUTED BY HASH(`id`) BUCKETS 3
   PROPERTIES ("replication_num" = "1");
   ```
4. Save table_map.json:
   ```json
   {
     "main_table": "main_data_table",
     "main_schema": "age|name"
   }
   ```

---

##### **Subsequent File Processing** - Schema Validation
**When**: table_map.json exists

**Steps**:
1. Read expected schema from table_map.json: `"age|name"`
2. Calculate current schema from file: `"age|name"`
3. Compare:
   - **Match**: Continue loading
   - **Mismatch**: 
     - Save entire file to `/app/error_files/error_<filename>.csv`
     - Log schema mismatch
     - Checkpoint the file (don't retry)
     - Skip to next file

**Example Mismatch**:
```
Expected: "age|name"
Got:      "name|salary"
â†’ Different columns!
â†’ Save to error_files/error_b.csv
â†’ Skip file
```

---

##### **Row-Level Validation** - Type Checking
**Purpose**: Filter out rows with invalid data types

**Algorithm**:
```python
FOR EACH ROW in dataframe:
    good_row = True
    validated_values = []
    
    FOR EACH COLUMN:
        expected_type = schema[column]  # From DESC table
        value = row[column]
        
        IF value is NULL:
            validated_values.append(NULL)
            
        ELSE IF expected_type is INT/BIGINT/TINYINT:
            TRY:
                int_value = int(float(str(value)))
                validated_values.append(int_value)
            EXCEPT:
                good_row = False
                Log: "Row X invalid: Column 'age' expects INT, got 'twenty'"
                BREAK
                
        ELSE IF expected_type is FLOAT/DOUBLE:
            TRY:
                float_value = float(str(value))
                validated_values.append(float_value)
            EXCEPT:
                good_row = False
                BREAK
                
        ELSE IF expected_type is DATE/DATETIME:
            TRY:
                date_value = pd.to_datetime(value)
                validated_values.append(value)
            EXCEPT:
                good_row = False
                BREAK
                
        ELSE:  # VARCHAR
            validated_values.append(value)  # Accept anything
    
    IF good_row:
        good_rows.append(tuple(validated_values))
    ELSE:
        bad_rows.append(row)
```

**Example**:
```
Row 1: (1, 'lara', 77)    â†’ age=77 converts to int âœ“ â†’ GOOD
Row 2: (2, 'Aara', 21)    â†’ age=21 converts to int âœ“ â†’ GOOD
Row 3: (3, 'aysuh', 'twenty') â†’ age='twenty' FAILS int conversion âœ— â†’ BAD
Row 4: (4, 'rahul', 'fifty')  â†’ age='fifty' FAILS int conversion âœ— â†’ BAD

Result:
  good_rows = [(1, 'lara', 77), (2, 'Aara', 21)]
  bad_rows = [(3, 'aysuh', 'twenty'), (4, 'rahul', 'fifty')]
  
  Save bad_rows to: error_files/error_e.csv
  Load only good_rows to database
```

---

##### **Database Insert** - Batch Loading
**Purpose**: Insert validated rows into Doris

**Steps**:
1. Get max(id) from table to continue auto-increment
2. Add id column to dataframe: `range(last_id+1, last_id+1+len(df))`
3. Build SQL:
   ```sql
   INSERT INTO `main_data_table` (`id`, `name`, `age`) 
   VALUES (%s, %s, %s)
   ```
4. Execute batch insert:
   ```python
   cursor.executemany(sql, good_rows)
   conn.commit()
   ```
5. Log success with row counts

---

#### **6_checkpoint.py** - Progress Tracking
**Role**: Mark file as processed to prevent reprocessing

**Input**: Filename (e.g., "a.csv")

**Logic**:
```python
with open('/app/checkpoint.txt', 'a') as f:
    f.write(f"{filename}\n")
```

**Output**: 
- Appends filename to checkpoint.txt
- File contents after processing 3 files:
  ```
  a.csv
  c.csv
  d.csv
  ```

---

## ğŸ“Š Data Flow & Transformations

### **Example: Processing e.csv**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ORIGINAL FILE: data/e.csv                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ name     | age                                               â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€                                              â”‚
â”‚ lara     | 77                                                â”‚
â”‚ Aara     | 21                                                â”‚
â”‚ aysuh    | twenty  â† Invalid (string in age column)         â”‚
â”‚ rahul    | fifty   â† Invalid (string in age column)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ [VALIDATE] âœ“ File valid
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRANSFORM: 3_transform.py                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - Clean column names: 'age ' â†’ 'age' (trim space)           â”‚
â”‚ - Remove duplicates: 0 found                                 â”‚
â”‚ - Fill NULLs: 0 found                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGED FILE: stage_test/staged_e.csv                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ name     | age                                               â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€                                              â”‚
â”‚ lara     | 77                                                â”‚
â”‚ Aara     | 21                                                â”‚
â”‚ aysuh    | twenty                                            â”‚
â”‚ rahul    | fifty                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ [LOAD TO DORIS]
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCHEMA CHECK                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Expected schema: "age|name" (from table_map.json)           â”‚
â”‚ Current schema:  "age|name"                                  â”‚
â”‚ Match? âœ“ YES â†’ Continue                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ADD AUTO-INCREMENT IDs                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ last_id from DB = 22 (from previous files)                  â”‚
â”‚                                                               â”‚
â”‚ id  | name     | age                                         â”‚
â”‚â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€                                        â”‚
â”‚ 23  | lara     | 77                                          â”‚
â”‚ 24  | Aara     | 21                                          â”‚
â”‚ 25  | aysuh    | twenty                                      â”‚
â”‚ 26  | rahul    | fifty                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ROW-LEVEL VALIDATION (Against Table Schema)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Table schema from Doris:                                     â”‚
â”‚   id: BIGINT                                                 â”‚
â”‚   name: VARCHAR(100)                                         â”‚
â”‚   age: TINYINT                                               â”‚
â”‚                                                               â”‚
â”‚ Row 1: (23, 'lara', 77)                                      â”‚
â”‚   âœ“ id=23 â†’ int âœ“                                           â”‚
â”‚   âœ“ name='lara' â†’ varchar âœ“                                 â”‚
â”‚   âœ“ age=77 â†’ int(float('77')) = 77 âœ“                        â”‚
â”‚   â†’ GOOD ROW                                                 â”‚
â”‚                                                               â”‚
â”‚ Row 2: (24, 'Aara', 21)                                      â”‚
â”‚   âœ“ id=24 â†’ int âœ“                                           â”‚
â”‚   âœ“ name='Aara' â†’ varchar âœ“                                 â”‚
â”‚   âœ“ age=21 â†’ int(float('21')) = 21 âœ“                        â”‚
â”‚   â†’ GOOD ROW                                                 â”‚
â”‚                                                               â”‚
â”‚ Row 3: (25, 'aysuh', 'twenty')                               â”‚
â”‚   âœ“ id=25 â†’ int âœ“                                           â”‚
â”‚   âœ“ name='aysuh' â†’ varchar âœ“                                â”‚
â”‚   âœ— age='twenty' â†’ int(float('twenty')) â†’ ValueError!       â”‚
â”‚   â†’ BAD ROW                                                  â”‚
â”‚   [WARN] Row 3 invalid: Column 'age' expects INT, got 'twenty'â”‚
â”‚                                                               â”‚
â”‚ Row 4: (26, 'rahul', 'fifty')                                â”‚
â”‚   âœ“ id=26 â†’ int âœ“                                           â”‚
â”‚   âœ“ name='rahul' â†’ varchar âœ“                                â”‚
â”‚   âœ— age='fifty' â†’ int(float('fifty')) â†’ ValueError!         â”‚
â”‚   â†’ BAD ROW                                                  â”‚
â”‚   [WARN] Row 4 invalid: Column 'age' expects INT, got 'fifty'â”‚
â”‚                                                               â”‚
â”‚ RESULT:                                                       â”‚
â”‚   Good rows: 2                                               â”‚
â”‚   Bad rows: 2                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚                     â”‚
                    Good Rows                Bad Rows
                            â”‚                     â”‚
                            â†“                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INSERT TO DORIS DATABASE     â”‚  â”‚ ERROR FILE:                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚ error_files/error_e.csv       â”‚
â”‚ INSERT INTO main_data_table  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ VALUES                        â”‚  â”‚ name     | age               â”‚
â”‚   (23, 'lara', 77),          â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€              â”‚
â”‚   (24, 'Aara', 21);          â”‚  â”‚ aysuh    | twenty            â”‚
â”‚                               â”‚  â”‚ rahul    | fifty             â”‚
â”‚ Rows inserted: 2              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CHECKPOINT: 6_checkpoint.py                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Append "e.csv" to checkpoint.txt                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
                      FILE COMPLETE!
```

---

## ğŸš¨ Error Handling Flow

```
                         ERROR DECISION TREE
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                               â”‚
         File Level Errors              Row Level Errors
                â”‚                               â”‚
                â†“                               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ VALIDATION FAILURE    â”‚      â”‚ TYPE MISMATCH         â”‚
    â”‚ (2_validate.py)       â”‚      â”‚ (4_load_to_doris.py)  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ - File not found      â”‚      â”‚ - String in INT col   â”‚
    â”‚ - Not a valid CSV     â”‚      â”‚ - Invalid number      â”‚
    â”‚ - Empty file          â”‚      â”‚ - Invalid date format â”‚
    â”‚ - No columns          â”‚      â”‚ - Type conversion err â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                              â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ CHECKPOINT FILE?     â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚ YES - Don't retry    â”‚
            â”‚ NO  - Will retry     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ SAVE TO error_files/         â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ Filename format:              â”‚
        â”‚   error_<original_name>.csv  â”‚
        â”‚                               â”‚
        â”‚ Examples:                     â”‚
        â”‚   error_e.csv                â”‚
        â”‚   error_b.csv                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ LOG ERROR   â”‚
                â”‚ CONTINUE    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                SCHEMA MISMATCH FLOW
                        â”‚
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ First file (table doesn't exist)  â”‚
        â”‚ â†’ Create table with detected typesâ”‚
        â”‚ â†’ Save schema to table_map.json   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Subsequent file                    â”‚
        â”‚ â†’ Read expected schema             â”‚
        â”‚ â†’ Compare with current file schema â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                       â”‚
        Match                   Mismatch
            â”‚                       â”‚
            â†“                       â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ CONTINUE     â”‚    â”‚ SAVE ENTIRE FILE       â”‚
    â”‚ Loading      â”‚    â”‚ to error_files/        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚ - Log schema mismatch  â”‚
                        â”‚ - Checkpoint file      â”‚
                        â”‚ - Skip to next file    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Error File Examples**

**error_e.csv** (Bad rows):
```csv
name,age
aysuh,twenty
rahul,fifty
```

**error_b.csv** (Schema mismatch - entire file):
```csv
name,salary
John,50000
Jane,60000
Bob,55000
```

**error_meal_metadata.csv** (Schema mismatch - different number of columns):
```csv
age,gender,weight_kg,...(54 columns total)
25,Male,75,...
```

---

## âš™ï¸ Kubernetes & Argo Integration

### **How Cron Scheduling Works**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ARGO WORKFLOWS ARCHITECTURE                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Minikube Cluster
â”œâ”€â”€ Namespace: argo
â”‚   â”œâ”€â”€ Argo Workflow Controller (Deployment)
â”‚   â”‚   â””â”€â”€ Pod: workflow-controller-xxxxx
â”‚   â”‚       â”œâ”€â”€ Watches CronWorkflows
â”‚   â”‚       â”œâ”€â”€ Creates Workflows on schedule
â”‚   â”‚       â””â”€â”€ Manages workflow lifecycle
â”‚   â”‚
â”‚   â”œâ”€â”€ CronWorkflow: csv-doris-cron (Custom Resource)
â”‚   â”‚   â””â”€â”€ spec:
â”‚   â”‚       â”œâ”€â”€ schedule: "*/5 * * * *"
â”‚   â”‚       â”œâ”€â”€ timezone: "America/New_York"
â”‚   â”‚       â””â”€â”€ workflowSpec: {...}
â”‚   â”‚
â”‚   â””â”€â”€ Workflows (Created dynamically every 5 minutes)
â”‚       â”œâ”€â”€ csv-doris-cron-1762411500 (Completed)
â”‚       â”œâ”€â”€ csv-doris-cron-1762411800 (Completed)
â”‚       â””â”€â”€ csv-doris-cron-1762412100 (Running)
â”‚           â””â”€â”€ Pod: csv-doris-cron-1762412100
â”‚               â”œâ”€â”€ Init Container: Install dependencies
â”‚               â””â”€â”€ Main Container: Run pipeline
â”‚
â””â”€â”€ Namespace: default (not used in this project)
```

### **Timeline of a Single Cron Execution**

```
Time: 06:45:00 (New York Time)
  â”‚
  â”‚ Argo Controller: "Time to create new workflow!"
  â†“
06:45:00 - CREATE Workflow: csv-doris-cron-1762411500
  â”‚         â””â”€ Generate unique ID from timestamp
  â”‚
06:45:01 - CREATE Pod: csv-doris-cron-1762411500
  â”‚         â””â”€ Kubernetes Scheduler assigns to node
  â”‚
06:45:02 - PULL Image: python:3.11-slim
  â”‚         â””â”€ (If not cached, download from Docker Hub)
  â”‚
06:45:05 - START Init Container
  â”‚         â””â”€ pip install pandas pymysql requests numpy
  â”‚
06:45:20 - Init Complete (dependencies installed)
  â”‚
06:45:21 - START Main Container
  â”‚         â””â”€ python3 /app/scripts/pipeline_local.py
  â”‚
06:45:21 - Pipeline Running...
  â”‚         â”œâ”€ Discover files
  â”‚         â”œâ”€ Process a.csv
  â”‚         â”œâ”€ Process b.csv (schema mismatch â†’ error)
  â”‚         â”œâ”€ Process c.csv
  â”‚         â”œâ”€ Process d.csv
  â”‚         â”œâ”€ Process e.csv (2 bad rows â†’ error file)
  â”‚         â”œâ”€ Process emp.csv
  â”‚         â”œâ”€ Process meal_metadata.csv (schema mismatch â†’ error)
  â”‚         â””â”€ Process people.csv
  â”‚
06:46:46 - Pipeline Complete (exit code 0)
  â”‚
06:46:47 - Workflow Status: Succeeded âœ“
  â”‚
06:46:50 - DELETE Pod (logs saved in Argo)
  â”‚
06:46:51 - Workflow archived (kept for history)
  â”‚
  â†“
Wait until 06:50:00...
  â”‚
06:50:00 - Next execution begins!
```

### **Volume Mounting Explained**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WINDOWS HOST                                                  â”‚
â”‚ C:\Users\singh\Desktop\Minikube-Doris\                       â”‚
â”‚                                                                â”‚
â”‚ â”œâ”€â”€ data/                                                     â”‚
â”‚ â”‚   â”œâ”€â”€ a.csv                                                â”‚
â”‚ â”‚   â”œâ”€â”€ b.csv                                                â”‚
â”‚ â”‚   â””â”€â”€ ...                                                  â”‚
â”‚ â”œâ”€â”€ scripts/                                                  â”‚
â”‚ â”‚   â”œâ”€â”€ pipeline_local.py                                    â”‚
â”‚ â”‚   â”œâ”€â”€ 4_load_to_doris.py                                   â”‚
â”‚ â”‚   â””â”€â”€ ...                                                  â”‚
â”‚ â”œâ”€â”€ checkpoint.txt                                            â”‚
â”‚ â””â”€â”€ table_map.json                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ Minikube hostPath mount
                 â”‚ (Configured in YAML)
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MINIKUBE VM                                                   â”‚
â”‚ /Minikube-Doris/  (Same files, shared)                       â”‚
â”‚                                                                â”‚
â”‚ â”œâ”€â”€ data/                                                     â”‚
â”‚ â”œâ”€â”€ scripts/                                                  â”‚
â”‚ â”œâ”€â”€ checkpoint.txt                                            â”‚
â”‚ â””â”€â”€ table_map.json                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ Pod volumeMount
                 â”‚ (Mounted into container)
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POD CONTAINER                                                 â”‚
â”‚ /app/  (Read/write access to Windows files!)                 â”‚
â”‚                                                                â”‚
â”‚ â”œâ”€â”€ data/                                                     â”‚
â”‚ â”‚   â””â”€â”€ Python reads: pd.read_csv('/app/data/a.csv')        â”‚
â”‚ â”œâ”€â”€ scripts/                                                  â”‚
â”‚ â”‚   â””â”€â”€ Executes: python3 /app/scripts/pipeline_local.py    â”‚
â”‚ â”œâ”€â”€ checkpoint.txt                                            â”‚
â”‚ â”‚   â””â”€â”€ Updates persist to Windows!                          â”‚
â”‚ â””â”€â”€ table_map.json                                            â”‚
â”‚     â””â”€â”€ Changes visible on Windows!                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KEY BENEFIT: No need to rebuild Docker image when you edit scripts!
            Changes on Windows are immediately visible in pods.
```

### **Environment Variables Flow**

```yaml
# In argo-cron-pipeline.yaml
env:
  - name: DORIS_HOST
    value: "host.docker.internal"  # Special DNS from pod to host
  - name: DORIS_PORT
    value: "9030"
  - name: DORIS_DB
    value: "updated_test2"
```

```python
# In scripts (e.g., 4_load_to_doris.py)
import os

doris_host = os.getenv("DORIS_HOST", "localhost")
# Returns: "host.docker.internal"

doris_port = int(os.getenv("DORIS_PORT", "9030"))
# Returns: 9030

doris_db = os.getenv("DORIS_DB", "test")
# Returns: "updated_test2"

# Connect to Doris
conn = pymysql.connect(
    host=doris_host,  # host.docker.internal
    port=doris_port,  # 9030
    database=doris_db  # updated_test2
)
```

**host.docker.internal** resolves to:
- From pod: `192.168.29.181` (your Windows machine's IP on network)
- This allows pod to connect to Doris running outside Kubernetes

---

## ğŸ¯ Key Design Decisions

### **1. Why DUPLICATE KEY Model?**
```sql
DUPLICATE KEY(`id`)
```
- Allows duplicate rows (Doris doesn't enforce uniqueness)
- Fast inserts (no primary key lookups)
- Suitable for append-only data pipelines
- Auto-increment handled by application (not database)

### **2. Why Majority Voting for Type Detection?**
```python
if numeric_count / total_count > 0.5:
    # Treat as numeric
```
- Handles data quality issues (occasional bad values)
- Example: Column with [22, 33, "N/A", 44] â†’ 75% numeric â†’ Treat as INT
- Bad values get filtered to error file during row validation

### **3. Why Checkpoint After Error?**
```python
# Even if file fails, checkpoint it
run(f"python3 6_checkpoint.py \"{next_file}\"", "Checkpoint (error)")
```
- Prevents infinite retry loops
- User can inspect error file and decide to reprocess manually
- Keeps pipeline moving forward

### **4. Why Separate Staged Files?**
```python
staged_path = os.path.join(STAGE_DIR, f"staged_{filename}")
```
- Original files remain untouched
- Can debug by comparing original vs staged
- Staged files can be deleted after successful load

### **5. Why Volume Mount Instead of Docker Image?**
- **Fast iteration**: Edit Python script â†’ Immediately available in next pod
- **No rebuild**: No need to rebuild/push Docker image for code changes
- **Live data**: CSV files added to folder are immediately processable
- **Debugging**: Can view checkpoint.txt, error files in real-time

---

## ğŸ“ˆ Performance Characteristics

**Pipeline Metrics** (from logs):
- Total runtime: ~60 seconds for 8 files
- Files processed: 6 successful
- Schema mismatches: 2 files
- Bad rows filtered: 2 rows
- Total rows loaded: 30 rows

**Breakdown by Stage**:
```
Stage                  | Time per File | % of Total
-----------------------|---------------|------------
Discover & Validate    | ~2-3 sec     | 10%
Transform              | ~2-3 sec     | 10%
Load to Doris          | ~3-5 sec     | 60%
Checkpoint             | ~1 sec       | 5%
Overhead (subprocess)  | ~1-2 sec     | 15%
```

**Bottlenecks**:
1. **Doris Connection**: Each file creates new connection (could use connection pooling)
2. **Subprocess Overhead**: Running 5+ Python scripts per file (could consolidate)
3. **Row-by-row Validation**: O(n) for each row (acceptable for small datasets)

**Scalability**:
- Current: 8 files, ~200 total rows â†’ 60 seconds
- Estimated: 100 files, ~10,000 rows â†’ 10-15 minutes
- Limit: Single-threaded, sequential processing

---

## ğŸ” Monitoring & Debugging

### **View Live Logs**
```powershell
# Follow logs from all workflow pods
kubectl logs -n argo -l workflows.argoproj.io/workflow --tail=200 -f

# View specific workflow
argo logs -n argo csv-doris-cron-1762411500 -f

# List recent workflows
argo list -n argo

# Get workflow details
argo get -n argo csv-doris-cron-1762411500
```

### **Check Files**
```powershell
# View checkpoint
Get-Content C:\Users\singh\Desktop\Minikube-Doris\checkpoint.txt

# View error files
Get-ChildItem C:\Users\singh\Desktop\Minikube-Doris\error_files

# View staged files
Get-ChildItem C:\Users\singh\Desktop\Minikube-Doris\stage_test

# View table map
Get-Content C:\Users\singh\Desktop\Minikube-Doris\table_map.json
```

### **Check Database**
```sql
-- Connect to Doris
mysql -h 192.168.29.181 -P 9030 -u root

-- View table schema
DESC updated_test2.main_data_table;

-- Count rows
SELECT COUNT(*) FROM updated_test2.main_data_table;

-- View sample data
SELECT * FROM updated_test2.main_data_table LIMIT 10;

-- Check data types are correct
SELECT 
  name,
  age,
  TYPEOF(age) as age_type  -- Should show TINYINT
FROM updated_test2.main_data_table
LIMIT 5;
```

---

## ğŸ“ Summary

**This pipeline demonstrates**:
1. âœ… Kubernetes-native workflow orchestration (Argo Workflows)
2. âœ… Cron-based scheduling (every 5 minutes)
3. âœ… Intelligent schema detection (majority voting algorithm)
4. âœ… Row-level data validation (type checking)
5. âœ… Error handling (schema mismatches, bad rows)
6. âœ… Incremental processing (checkpoint-based)
7. âœ… Clean separation of concerns (6 specialized scripts)
8. âœ… Production-ready logging (timestamps, progress, summaries)
9. âœ… Volume mounting for live code/data updates
10. âœ… MySQL protocol for database loading

**Perfect for**:
- ETL pipelines with varying CSV schemas
- Data quality enforcement
- Automated data ingestion
- Multi-tenant data loading (one table for compatible schemas)

---

*End of Documentation*
